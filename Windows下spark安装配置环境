# 安装主要是版本的问题
1、安装jdk，配置环境变量，在cmd中javac和java --verion命令都要有结果才能算配置完成。
2、安装spark，spark版本要和集群版本相匹配，下好tgz文件后直接解压，并设置系统变量path和SPARK_HOME，在cmd中spark-shell有结果才可以使用.
3、python版本跟spark版本兼容，spark 2.x不兼容python3.8
4、pyspark版本问题不大，使用最新的即可。
5、使用自定义函数需要考虑pandas和numpy版本
